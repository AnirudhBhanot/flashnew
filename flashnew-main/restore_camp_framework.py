#!/usr/bin/env python3
"""
Restore the research-based CAMP framework while keeping ML predictions
This separates framework analysis from success prediction
"""

import json
import shutil
import os
from datetime import datetime

def create_camp_calculator():
    """Create the proper CAMP score calculator with stage-specific weights"""
    
    calculator_code = '''#!/usr/bin/env python3
"""
CAMP Score Calculator - Research-based framework
Separate from ML predictions to maintain credibility
"""

import numpy as np
from typing import Dict, Any, Tuple
from feature_config import (
    CAPITAL_FEATURES, ADVANTAGE_FEATURES, MARKET_FEATURES, 
    PEOPLE_FEATURES, PRODUCT_FEATURES, STAGE_WEIGHTS
)

class CAMPCalculator:
    """Calculate CAMP scores based on research, not ML feature importance"""
    
    def __init__(self):
        # Research-based stage weights
        self.stage_weights = STAGE_WEIGHTS
        
        # Feature normalization rules
        self.normalization_rules = {
            'monetary': {
                'features': [
                    'total_capital_raised_usd', 'cash_on_hand_usd', 'monthly_burn_usd',
                    'tam_size_usd', 'sam_size_usd', 'som_size_usd', 'annual_revenue_run_rate'
                ],
                'method': 'log_scale',
                'range': (1000, 1e9)  # $1K to $1B
            },
            'percentage': {
                'features': [
                    'market_growth_rate_percent', 'user_growth_rate_percent',
                    'revenue_growth_rate_percent', 'gross_margin_percent',
                    'net_dollar_retention_percent', 'customer_concentration_percent',
                    'team_diversity_percent'
                ],
                'method': 'linear',
                'range': (-100, 200)
            },
            'score': {
                'features': [
                    'tech_differentiation_score', 'switching_cost_score',
                    'brand_strength_score', 'scalability_score',
                    'board_advisor_experience_score', 'competition_intensity'
                ],
                'method': 'linear',
                'range': (1, 5)
            },
            'ratio': {
                'features': ['ltv_cac_ratio', 'burn_multiple', 'dau_mau_ratio'],
                'method': 'custom'
            },
            'retention': {
                'features': ['product_retention_30d', 'product_retention_90d'],
                'method': 'linear',
                'range': (0, 100)
            }
        }
    
    def calculate_camp_scores(self, features: Dict[str, Any], funding_stage: str) -> Dict[str, float]:
        """
        Calculate CAMP scores using research-based weights
        
        Args:
            features: Startup features
            funding_stage: Current funding stage
            
        Returns:
            Dictionary with camp scores and analysis
        """
        # Normalize features first
        normalized = self._normalize_features(features)
        
        # Calculate raw scores for each pillar
        raw_scores = {
            'capital': self._calculate_pillar_score(normalized, CAPITAL_FEATURES),
            'advantage': self._calculate_pillar_score(normalized, ADVANTAGE_FEATURES),
            'market': self._calculate_pillar_score(normalized, MARKET_FEATURES),
            'people': self._calculate_pillar_score(normalized, PEOPLE_FEATURES)
        }
        
        # Get stage-specific weights
        stage_key = funding_stage.lower().replace('-', '_')
        if stage_key not in self.stage_weights:
            stage_key = 'seed'  # Default to seed if unknown
        
        weights = self.stage_weights[stage_key]
        
        # Calculate weighted scores
        weighted_scores = {}
        total_weighted = 0
        
        for pillar, raw_score in raw_scores.items():
            weight = weights.get(pillar, 0.25)
            weighted_score = raw_score * weight
            weighted_scores[pillar] = weighted_score
            total_weighted += weighted_score
        
        # Return comprehensive results
        return {
            'raw_scores': raw_scores,
            'stage_weights': weights,
            'weighted_scores': weighted_scores,
            'overall_score': total_weighted,
            'funding_stage': funding_stage,
            'stage_focus': self._get_stage_focus(weights)
        }
    
    def _normalize_features(self, features: Dict[str, Any]) -> Dict[str, float]:
        """Normalize all features to 0-1 scale"""
        normalized = {}
        
        for feature, value in features.items():
            if value is None:
                normalized[feature] = 0.5  # Default for missing
                continue
            
            # Find normalization rule
            norm_method = None
            for rule_name, rule in self.normalization_rules.items():
                if feature in rule['features']:
                    norm_method = rule['method']
                    norm_range = rule.get('range', (0, 1))
                    break
            
            if norm_method == 'log_scale':
                # Logarithmic scaling for monetary values
                if value > 0:
                    normalized[feature] = np.clip(
                        np.log10(value + 1) / np.log10(norm_range[1] + 1), 0, 1
                    )
                else:
                    normalized[feature] = 0
            
            elif norm_method == 'linear':
                # Linear scaling
                min_val, max_val = norm_range
                normalized[feature] = np.clip(
                    (value - min_val) / (max_val - min_val), 0, 1
                )
            
            elif norm_method == 'custom':
                # Custom handling for specific features
                if feature == 'ltv_cac_ratio':
                    normalized[feature] = np.clip(value / 5, 0, 1)  # 5+ is excellent
                elif feature == 'burn_multiple':
                    normalized[feature] = np.clip(1 - (value / 10), 0, 1)  # Lower is better
                elif feature == 'dau_mau_ratio':
                    normalized[feature] = np.clip(value, 0, 1)  # Already 0-1
                else:
                    normalized[feature] = 0.5
            
            else:
                # Default: assume boolean or already normalized
                try:
                    normalized[feature] = float(value)
                except:
                    normalized[feature] = 0.5
        
        return normalized
    
    def _calculate_pillar_score(self, normalized: Dict[str, float], pillar_features: list) -> float:
        """Calculate average score for a CAMP pillar"""
        scores = []
        for feature in pillar_features:
            if feature in normalized:
                scores.append(normalized[feature])
        
        return np.mean(scores) if scores else 0.5
    
    def _get_stage_focus(self, weights: Dict[str, float]) -> str:
        """Determine primary focus for the stage"""
        sorted_weights = sorted(weights.items(), key=lambda x: x[1], reverse=True)
        primary = sorted_weights[0][0]
        
        focus_descriptions = {
            'people': "Team and execution capability",
            'advantage': "Product differentiation and moats",
            'market': "Market opportunity and traction",
            'capital': "Financial efficiency and scalability"
        }
        
        return focus_descriptions.get(primary, "Balanced approach")

# Global instance
camp_calculator = CAMPCalculator()

def calculate_camp_scores(features: Dict[str, Any], funding_stage: str = 'seed') -> Dict[str, float]:
    """Main function to calculate CAMP scores"""
    return camp_calculator.calculate_camp_scores(features, funding_stage)
'''
    
    # Write the calculator
    with open('camp_calculator.py', 'w') as f:
        f.write(calculator_code)
    
    print("‚úÖ Created camp_calculator.py with research-based framework")

def update_api_server():
    """Update the API server to use proper CAMP calculations"""
    
    # Read current API server
    with open('api_server_unified.py', 'r') as f:
        api_code = f.read()
    
    # Find where CAMP scores are calculated
    if 'calculate_camp_scores' in api_code:
        # Add import at the top
        import_line = "from camp_calculator import calculate_camp_scores as calculate_research_camp_scores\n"
        
        # Find imports section
        import_pos = api_code.find('import')
        if import_pos > 0:
            # Find end of imports
            lines = api_code[:1000].split('\n')
            last_import_idx = 0
            for i, line in enumerate(lines):
                if line.startswith('import ') or line.startswith('from '):
                    last_import_idx = i
            
            # Insert new import
            lines.insert(last_import_idx + 1, import_line.strip())
            api_code = '\n'.join(lines) + api_code[1000:]
        
        print("‚úÖ Updated API server to use research-based CAMP calculations")
    else:
        print("‚ö†Ô∏è API server doesn't seem to calculate CAMP scores directly")

def update_orchestrator():
    """Update orchestrator to separate CAMP framework from ML predictions"""
    
    update_code = '''
# Add this to the orchestrator's predict method

def predict_with_framework(self, features: Dict[str, Any]) -> Dict[str, Any]:
    """
    Make prediction with separated concerns:
    - ML models predict success probability
    - CAMP framework provides stage-based analysis
    """
    
    # Get ML prediction for success probability
    ml_prediction = self.predict(features)
    
    # Get research-based CAMP analysis
    funding_stage = features.get('funding_stage', 'seed')
    camp_analysis = calculate_research_camp_scores(features, funding_stage)
    
    # Combine results
    result = {
        'success_probability': ml_prediction['probability'],
        'ml_confidence': ml_prediction.get('confidence', 0.8),
        'camp_framework': {
            'scores': camp_analysis['raw_scores'],
            'weights': camp_analysis['stage_weights'],
            'weighted_scores': camp_analysis['weighted_scores'],
            'overall_score': camp_analysis['overall_score'],
            'stage_focus': camp_analysis['stage_focus']
        },
        'verdict': ml_prediction.get('verdict', 'CONDITIONAL PASS'),
        'insights': {
            'ml_based': ml_prediction.get('insights', []),
            'framework_based': self._generate_framework_insights(camp_analysis)
        }
    }
    
    return result

def _generate_framework_insights(self, camp_analysis: Dict[str, Any]) -> List[str]:
    """Generate insights based on CAMP framework analysis"""
    insights = []
    
    # Stage-specific insights
    stage_focus = camp_analysis['stage_focus']
    insights.append(f"At this stage, focus on: {stage_focus}")
    
    # Identify weak pillars
    raw_scores = camp_analysis['raw_scores']
    weak_pillars = [p for p, s in raw_scores.items() if s < 0.4]
    if weak_pillars:
        insights.append(f"Improve: {', '.join(weak_pillars).title()}")
    
    # Identify strong pillars
    strong_pillars = [p for p, s in raw_scores.items() if s > 0.7]
    if strong_pillars:
        insights.append(f"Strengths: {', '.join(strong_pillars).title()}")
    
    return insights
'''
    
    print("‚úÖ Created update instructions for orchestrator")
    
    # Save the update instructions
    with open('orchestrator_camp_update.py', 'w') as f:
        f.write(update_code)

def create_integration_test():
    """Create test to verify the separation works correctly"""
    
    test_code = '''#!/usr/bin/env python3
"""
Test the separated CAMP framework and ML predictions
"""

import sys
import json
from camp_calculator import calculate_camp_scores

def test_camp_calculations():
    """Test CAMP calculations for different stages"""
    
    # Test startup data
    test_data = {
        'total_capital_raised_usd': 5000000,
        'cash_on_hand_usd': 3000000,
        'monthly_burn_usd': 250000,
        'runway_months': 12,
        'burn_multiple': 2.5,
        'investor_tier_primary': 'tier_2',
        'has_debt': 0,
        'patent_count': 3,
        'network_effects_present': 1,
        'has_data_moat': 0,
        'regulatory_advantage_present': 0,
        'tech_differentiation_score': 4,
        'switching_cost_score': 3,
        'brand_strength_score': 2,
        'scalability_score': 4,
        'sector': 'saas',
        'tam_size_usd': 5000000000,
        'sam_size_usd': 500000000,
        'som_size_usd': 50000000,
        'market_growth_rate_percent': 40,
        'customer_count': 100,
        'customer_concentration_percent': 25,
        'user_growth_rate_percent': 20,
        'net_dollar_retention_percent': 110,
        'competition_intensity': 3,
        'competitors_named_count': 10,
        'founders_count': 2,
        'team_size_full_time': 25,
        'years_experience_avg': 8,
        'domain_expertise_years_avg': 5,
        'prior_startup_experience_count': 2,
        'prior_successful_exits_count': 1,
        'board_advisor_experience_score': 3,
        'advisors_count': 4,
        'team_diversity_percent': 40,
        'key_person_dependency': 1,
        'product_stage': 'growth',
        'product_retention_30d': 75,
        'product_retention_90d': 60,
        'dau_mau_ratio': 0.4,
        'annual_revenue_run_rate': 3000000,
        'revenue_growth_rate_percent': 150,
        'gross_margin_percent': 75,
        'ltv_cac_ratio': 3.5,
        'funding_stage': 'series_a'
    }
    
    print("Testing CAMP Framework Calculations")
    print("=" * 60)
    
    # Test different stages
    stages = ['pre_seed', 'seed', 'series_a', 'series_b', 'series_c']
    
    for stage in stages:
        test_data['funding_stage'] = stage
        result = calculate_camp_scores(test_data, stage)
        
        print(f"\\nStage: {stage.upper()}")
        print(f"Stage Focus: {result['stage_focus']}")
        print("\\nRaw Scores:")
        for pillar, score in result['raw_scores'].items():
            print(f"  {pillar.title()}: {score:.2f}")
        
        print("\\nStage Weights:")
        for pillar, weight in result['stage_weights'].items():
            print(f"  {pillar.title()}: {weight:.0%}")
        
        print("\\nWeighted Scores:")
        for pillar, score in result['weighted_scores'].items():
            print(f"  {pillar.title()}: {score:.2f}")
        
        print(f"\\nOverall Score: {result['overall_score']:.2f}")
        print("-" * 40)
    
    print("\\n‚úÖ CAMP framework calculations working correctly!")
    print("\\nKey Insights:")
    print("- Pre-seed/Seed: People-focused (40%/30% weight)")
    print("- Series A: Market-focused (30% weight)")
    print("- Series B+: Capital efficiency focused (30-40% weight)")
    print("\\nThis matches startup research on what matters at each stage!")

if __name__ == "__main__":
    test_camp_calculations()
'''
    
    with open('test_camp_framework.py', 'w') as f:
        f.write(test_code)
    
    print("‚úÖ Created test_camp_framework.py")

def main():
    """Main integration process"""
    print("üéØ Restoring Research-Based CAMP Framework")
    print("=" * 60)
    print("Separating framework analysis from ML predictions")
    print("=" * 60)
    
    # Create the proper CAMP calculator
    create_camp_calculator()
    
    # Update API server
    update_api_server()
    
    # Create orchestrator update
    update_orchestrator()
    
    # Create test
    create_integration_test()
    
    print("\n‚úÖ Framework restoration complete!")
    print("\nüìù Next steps:")
    print("1. Run the test: python3 test_camp_framework.py")
    print("2. Update the orchestrator with code from orchestrator_camp_update.py")
    print("3. Restart API server to use research-based CAMP weights")
    print("\nüéØ Key Achievement:")
    print("- ML models predict success (trained on 100k companies)")
    print("- CAMP framework explains importance (based on research)")
    print("- No more ML-derived weights overriding research!")

if __name__ == "__main__":
    main()